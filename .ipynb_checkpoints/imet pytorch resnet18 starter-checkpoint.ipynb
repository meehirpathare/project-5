{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/yasufuminakama/imet-2020-pytorch-resnet18-starter/notebook?scriptVersionId=30968669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_path = \"D:\\\\METIS\\\\kaggle\\\\imet-2020-fgvc7\\\\\"\n",
    "data_path = \"D:\\\\METIS\\\\data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels.csv', 'sample_submission.csv', 'test', 'train.csv', 'train']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(met_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(met_path + 'train.csv')\n",
    "labels = pd.read_csv(met_path + 'labels.csv')\n",
    "submission = pd.read_csv(met_path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>attribute_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000040d66f14ced4cdd18cd95d91800f</td>\n",
       "      <td>448 2429 782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000ef13e37ef70412166725ec034a8a</td>\n",
       "      <td>2997 3231 2730 3294 3099 2017 784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001eeb4a06e8daa7c6951bcd124c3c7</td>\n",
       "      <td>2436 1715 23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000226398d224de78b191e6db45fd94e</td>\n",
       "      <td>2997 3433 448 782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00029c3b0171158d63b1bbf803a7d750</td>\n",
       "      <td>3465 3322 3170 1553 781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                      attribute_ids\n",
       "0  000040d66f14ced4cdd18cd95d91800f                       448 2429 782\n",
       "1  0000ef13e37ef70412166725ec034a8a  2997 3231 2730 3294 3099 2017 784\n",
       "2  0001eeb4a06e8daa7c6951bcd124c3c7                       2436 1715 23\n",
       "3  000226398d224de78b191e6db45fd94e                  2997 3433 448 782\n",
       "4  00029c3b0171158d63b1bbf803a7d750            3465 3322 3170 1553 781"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_id</th>\n",
       "      <th>attribute_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>country::afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>country::alamania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>country::algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>country::arabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>country::armenia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attribute_id        attribute_name\n",
       "0             0  country::afghanistan\n",
       "1             1     country::alamania\n",
       "2             2      country::algeria\n",
       "3             3       country::arabia\n",
       "4             4      country::armenia"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3474"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['attribute_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3471\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cls_counts = Counter(cls for classes in train['attribute_ids'].str.split() for cls in classes)\n",
    "\n",
    "print(len(cls_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute_id: 1006  attribute_name: medium::buckshorn\n",
      "attribute_id: 1961  attribute_name: medium::paper fringe\n",
      "attribute_id: 676  attribute_name: culture::southern german or swiss\n"
     ]
    }
   ],
   "source": [
    "label_map = dict(labels[['attribute_id', 'attribute_name']].values.tolist())\n",
    "not_in_train_labels = set(labels['attribute_id'].astype(str).values) - set(list(cls_counts))\n",
    "for _id in not_in_train_labels:\n",
    "    label = label_map[int(_id)]\n",
    "    print(f'attribute_id: {_id}  attribute_name: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute_name: tags::men  count: 21784\n",
      "attribute_name: dimension::large  count: 21142\n",
      "attribute_name: dimension::medium  count: 20672\n",
      "attribute_name: dimension::very large  count: 20657\n",
      "attribute_name: dimension::small  count: 19881\n",
      "attribute_name: dimension::tiny  count: 19602\n",
      "attribute_name: tags::women  count: 15840\n",
      "attribute_name: medium::wood  count: 11653\n",
      "attribute_name: medium::terracotta  count: 11636\n",
      "attribute_name: culture::japan  count: 11077\n",
      "attribute_name: medium::silk  count: 10589\n",
      "attribute_name: culture::greek  count: 10227\n",
      "attribute_name: country::egypt  count: 9746\n",
      "attribute_name: tags::flowers  count: 9498\n",
      "attribute_name: medium::silver  count: 8699\n",
      "attribute_name: culture::attic  count: 8630\n",
      "attribute_name: culture::china  count: 7723\n",
      "attribute_name: tags::portraits  count: 7527\n",
      "attribute_name: culture::french  count: 7457\n",
      "attribute_name: medium::gold  count: 7430\n"
     ]
    }
   ],
   "source": [
    "# TOP 20 common attribute\n",
    "for item in sorted(cls_counts.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    _id, count = item[0], item[1]\n",
    "    label = label_map[int(_id)]\n",
    "    print(f'attribute_name: {label}  count: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of labels for each instance')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of labels for each instance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_label_len = train.attribute_ids.str.split(\" \").apply(len)\n",
    "plt.figure(figsize=(25, 4))\n",
    "df_label_len.value_counts().plot.bar()\n",
    "plt.title(f\"Number of labels for each instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "\n",
    "import sys\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "\n",
    "from albumentations import Compose, Normalize, Resize, RandomResizedCrop\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n",
    "\n",
    "    \n",
    "def init_logger(log_file='train.log'):\n",
    "    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler\n",
    "    \n",
    "    log_format = '%(asctime)s %(levelname)s %(message)s'\n",
    "    \n",
    "    stream_handler = StreamHandler()\n",
    "    stream_handler.setLevel(DEBUG)\n",
    "    stream_handler.setFormatter(Formatter(log_format))\n",
    "    \n",
    "    file_handler = FileHandler(log_file)\n",
    "    file_handler.setFormatter(Formatter(log_format))\n",
    "    \n",
    "    logger = getLogger('Herbarium')\n",
    "    logger.setLevel(DEBUG)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "LOG_FILE = 'train.log'\n",
    "LOGGER = init_logger(LOG_FILE)\n",
    "\n",
    "\n",
    "def seed_torch(seed=777):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 777\n",
    "seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 3474\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, labels, transform=None):\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.df['id'].values[idx]\n",
    "        file_path = met_path + f'train\\\\{file_name}.png'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        label = self.labels.values[idx]\n",
    "        target = torch.zeros(N_CLASSES)\n",
    "        for cls in label.split():\n",
    "            target[int(cls)] = 1\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.df['id'].values[idx]\n",
    "        file_path = met_path + f'test\\\\{file_name}.png'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "\n",
    "\n",
    "def get_transforms(*, data):\n",
    "    \n",
    "    assert data in ('train', 'valid')\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            #Resize(HEIGHT, WIDTH),\n",
    "            RandomResizedCrop(HEIGHT, WIDTH),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(HEIGHT, WIDTH),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folds(df, n_folds, seed):\n",
    "    cls_counts = Counter(cls for classes in df['attribute_ids'].str.split() for cls in classes)\n",
    "    fold_cls_counts = defaultdict(int)\n",
    "    folds = [-1] * len(df)\n",
    "    for item in df.sample(frac=1, random_state=seed).itertuples():\n",
    "        cls = min(item.attribute_ids.split(), key=lambda cls: cls_counts[cls])\n",
    "        fold_counts = [(f, fold_cls_counts[f, cls]) for f in range(n_folds)]\n",
    "        min_count = min([count for _, count in fold_counts])\n",
    "        random.seed(item.Index)\n",
    "        fold = random.choice([f for f, count in fold_counts if count == min_count])\n",
    "        folds[item.Index] = fold\n",
    "        for cls in item.attribute_ids.split():\n",
    "            fold_cls_counts[fold, cls] += 1\n",
    "    df['fold'] = folds\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113706,) (28413,)\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "N_FOLDS = 5\n",
    "FOLD = 0\n",
    "\n",
    "if DEBUG:\n",
    "    folds = train.sample(n=10000, random_state=SEED).reset_index(drop=True).copy()\n",
    "    folds = make_folds(folds, N_FOLDS, SEED)\n",
    "else:\n",
    "    folds = train.copy()\n",
    "    folds = make_folds(folds, N_FOLDS, SEED)\n",
    "    \n",
    "trn_idx = folds[folds['fold'] != FOLD].index\n",
    "val_idx = folds[folds['fold'] == FOLD].index\n",
    "print(trn_idx.shape, val_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(folds.loc[trn_idx].reset_index(drop=True), \n",
    "                             folds.loc[trn_idx]['attribute_ids'], \n",
    "                             transform=get_transforms(data='train'))\n",
    "valid_dataset = TrainDataset(folds.loc[val_idx].reset_index(drop=True), \n",
    "                             folds.loc[val_idx]['attribute_ids'], \n",
    "                             transform=get_transforms(data='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "weights_path = data_path + 'resnet18\\\\resnet18.pth'\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "model.fc = nn.Linear(model.fc.in_features, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "\n",
    "def get_score(targets, y_pred):\n",
    "    return fbeta_score(targets, y_pred, beta=2, average='samples')\n",
    "\n",
    "\n",
    "\n",
    "def binarize_prediction(probabilities, threshold: float, argsorted=None,\n",
    "                        min_labels=1, max_labels=10):\n",
    "    \"\"\" \n",
    "    Return matrix of 0/1 predictions, same shape as probabilities.\n",
    "    \"\"\"\n",
    "    assert probabilities.shape[1] == N_CLASSES\n",
    "    if argsorted is None:\n",
    "        argsorted = probabilities.argsort(axis=1)\n",
    "    max_mask = _make_mask(argsorted, max_labels)\n",
    "    min_mask = _make_mask(argsorted, min_labels)\n",
    "    prob_mask = probabilities > threshold\n",
    "    return (max_mask & prob_mask) | min_mask\n",
    "\n",
    "\n",
    "def _make_mask(argsorted, top_n: int):\n",
    "    mask = np.zeros_like(argsorted, dtype=np.uint8)\n",
    "    col_indices = argsorted[:, -top_n:].reshape(-1)\n",
    "    row_indices = [i // top_n for i in range(len(col_indices))]\n",
    "    mask[row_indices, col_indices] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _reduce_loss(loss):\n",
    "    return loss.sum() / loss.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 14:43:55,421 INFO [Train model] start\n",
      "  0%|                                                                                          | 0/889 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.0.1) C:\\ci\\opencv-suite_1573470242804\\work\\modules\\imgproc\\src\\color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-5043337d7702>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mtk0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtk0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1165\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1166\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-7fd21bd0bae0>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'../input/imet-2020-fgvc7/train/{file_name}.png'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.0.1) C:\\ci\\opencv-suite_1573470242804\\work\\modules\\imgproc\\src\\color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "with timer('Train model'):\n",
    "    \n",
    "    n_epochs = 12\n",
    "    lr = 1e-4\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=lr, amsgrad=False)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.75, patience=4, verbose=True, eps=1e-6)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    best_score = 0.\n",
    "    best_thresh = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        tk0 = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        \n",
    "        for i, (images, labels) in tk0:\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_preds = model(images)\n",
    "            loss = _reduce_loss(criterion(y_preds, labels))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        preds = []\n",
    "                \n",
    "        valid_labels = []\n",
    "        tk1 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "\n",
    "        for i, (images, labels) in tk1:\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(images)\n",
    "\n",
    "            preds.append(torch.sigmoid(y_preds).to('cpu').numpy())\n",
    "            valid_labels.append(labels.to('cpu').numpy())\n",
    "\n",
    "            loss = _reduce_loss(criterion(y_preds, labels))\n",
    "            avg_val_loss += loss.item() / len(valid_loader)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        preds = np.concatenate(preds)\n",
    "        valid_labels = np.concatenate(valid_labels)\n",
    "        argsorted = preds.argsort(axis=1)\n",
    "\n",
    "        th_scores = {}\n",
    "        for threshold in [0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15]:\n",
    "            _score = get_score(valid_labels, binarize_prediction(preds, threshold, argsorted))\n",
    "            th_scores[threshold] = _score\n",
    "\n",
    "        max_kv = max(th_scores.items(), key=lambda x: x[1])\n",
    "        th, score = max_kv[0], max_kv[1]\n",
    "\n",
    "                        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.debug(f'  Epoch {epoch+1} - threshold: {th}  f2_score: {score}')\n",
    "\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_thresh = th\n",
    "            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model  threshold: {best_thresh}')\n",
    "            torch.save(model.state_dict(), f'fold{FOLD}_best_score.pth')\n",
    "\n",
    "        if avg_val_loss<best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "            torch.save(model.state_dict(), f'fold{FOLD}_best_loss.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
